{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dd29235-cd06-45aa-8cd2-fff4f1aed189",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Develope\\installation\\anaconda3\\envs\\machine_learning\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments, default_data_collator, AutoTokenizer\n",
    "from datasets import Dataset, load_metric\n",
    "import torch\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "import contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac56ac39-2637-4f4d-bdef-5b7181ec7962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded file: test.csv\n",
      "Loaded file: train.csv\n",
      "Loaded file: validation.csv\n"
     ]
    }
   ],
   "source": [
    "# DataSet Source\n",
    "# https://www.kaggle.com/datasets/gowrishankarp/newspaper-text-summarization-cnn-dailymail/data\n",
    "def load_input_dataset_files():\n",
    "    files = {}\n",
    "    for dirname, _, filenames in os.walk('./dataset'):\n",
    "        for filename in filenames:\n",
    "            fullpath = os.path.join(dirname, filename)\n",
    "            if filename.split(\".\")[-1] == \"csv\":\n",
    "                files[''.join(filename.split(\".\")[:-1])] = pd.read_csv(fullpath)\n",
    "                print(f\"Loaded file: {filename}\")\n",
    "    return files\n",
    "input_files = load_input_dataset_files()\n",
    "raw_train_all_df = input_files['train']\n",
    "raw_validation_all_df = input_files['validation']\n",
    "print(raw_train_all_df.shape)\n",
    "print(raw_validation_all_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76216d01-7f43-40e1-88ba-5c0d2b054250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       article_length  summary_length\n",
      "count   287113.000000   287113.000000\n",
      "mean       691.869494       51.574101\n",
      "std        336.500035       21.256336\n",
      "min          8.000000        4.000000\n",
      "25%        443.000000       38.000000\n",
      "50%        632.000000       48.000000\n",
      "75%        877.000000       60.000000\n",
      "max       2347.000000     1296.000000\n",
      "       article_length  summary_length\n",
      "count    93745.000000    93745.000000\n",
      "mean       359.090511       44.842648\n",
      "std         95.759521       15.906919\n",
      "min          8.000000        6.000000\n",
      "25%        292.000000       35.000000\n",
      "50%        373.000000       43.000000\n",
      "75%        439.000000       53.000000\n",
      "max        500.000000      467.000000\n"
     ]
    }
   ],
   "source": [
    "raw_train_all_df['article_length'] = raw_train_all_df['article'].apply(lambda x: len(x.split()))\n",
    "raw_train_all_df['summary_length'] = raw_train_all_df['highlights'].apply(lambda x: len(x.split()))\n",
    "raw_validation_all_df['article_length'] = raw_validation_all_df['article'].apply(lambda x: len(x.split()))\n",
    "raw_validation_all_df['summary_length'] = raw_validation_all_df['highlights'].apply(lambda x: len(x.split()))\n",
    "print(raw_train_all_df[['article_length', 'summary_length']].describe())\n",
    "\n",
    "raw_train_all_df['article_length'] = pd.to_numeric(raw_train_all_df['article_length'], errors='coerce')\n",
    "raw_train_all_df['summary_length'] = pd.to_numeric(raw_train_all_df['summary_length'], errors='coerce')\n",
    "\n",
    "raw_validation_all_df['article_length'] = pd.to_numeric(raw_validation_all_df['article_length'], errors='coerce')\n",
    "raw_validation_all_df['summary_length'] = pd.to_numeric(raw_validation_all_df['summary_length'], errors='coerce')\n",
    "\n",
    "train_all_df = raw_train_all_df[(raw_train_all_df['article_length'] <= 500) & (raw_train_all_df['summary_length'] <= 500)]\n",
    "validation_all_df = raw_validation_all_df[(raw_validation_all_df['article_length'] <= 500) & (raw_validation_all_df['summary_length'] <= 500)]\n",
    "print(train_all_df[['article_length', 'summary_length']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9327c6b0-9302-4183-9f7e-e10a18276635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(93745, 5)\n",
      "(4825, 5)\n"
     ]
    }
   ],
   "source": [
    "print(train_all_df.shape)\n",
    "print(validation_all_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06131601-37ec-4c7a-867c-fcb20b12e919",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_all_df.sample(n=80000, random_state=42)\n",
    "validation_df = validation_all_df.sample(n=4000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba64b638-bef6-4e1e-93eb-9153ceafd456",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    # Remove leading/trailing whitespace\n",
    "    text = text.strip()\n",
    "    # Replace multiple spaces with single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "def remove_unwanted_characters(text):\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Remove numbers (optional)\n",
    "    # text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove excess punctuation (e.g., !!!, ???)\n",
    "    text = re.sub(r'([!?.])\\1+', r'\\1', text)\n",
    "    \n",
    "    # Remove other unwanted symbols (customize as needed)\n",
    "    # Example: Remove pipe |, tilde ~, caret ^, etc.\n",
    "    text = re.sub(r'[|~^]', '', text)\n",
    "    return text\n",
    "\n",
    "def expand_contractions_text(text):\n",
    "    return contractions.fix(text)\n",
    "\n",
    "def standardize_possessives(text):\n",
    "    # Example: Convert \"US's\" to \"US is\" if appropriate\n",
    "    # Note: This is context-dependent and should be used carefully\n",
    "    text = re.sub(r\"(\\b\\w+)'s\\b\", r\"\\1 is\", text)\n",
    "    return text\n",
    "\n",
    "def format_all(df_list):\n",
    "    for df in df_list:\n",
    "        df.drop_duplicates(subset=['article', 'highlights'], inplace=True)\n",
    "\n",
    "        df['article'] = df['article'].apply(expand_contractions_text)\n",
    "        df['highlights'] = df['highlights'].apply(expand_contractions_text)\n",
    "\n",
    "        df['article'] = df['article'].apply(standardize_possessives)\n",
    "        df['highlights'] = df['highlights'].apply(standardize_possessives)\n",
    "        \n",
    "        df['article'] = df['article'].apply(normalize_text)\n",
    "        df['highlights'] = df['highlights'].apply(normalize_text)\n",
    "        \n",
    "        df['article'] = df['article'].apply(remove_unwanted_characters)\n",
    "        df['highlights'] = df['highlights'].apply(remove_unwanted_characters)\n",
    "\n",
    "        df['article'] = df['article'].str.lower()\n",
    "        df['highlights'] = df['highlights'].str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e8d0028-a992-4055-8f4a-0a21187f0e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_copy = train_df.copy()\n",
    "validation_df_copy = validation_df.copy()\n",
    "format_all([train_df_copy, validation_df_copy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acb84a20-c389-4210-a109-fda5094e4abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    inputs = examples['article']\n",
    "    model_inputs = tokenizer(inputs, max_length=512, padding='max_length', truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples['highlights'], max_length=150, padding='max_length', truncation=True)\n",
    "\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    rouge = load_metric(\"rouge\")\n",
    "    predictions, labels = eval_pred\n",
    "    # Decode the predicted token IDs to strings\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Decode the label token IDs to strings\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Compute ROUGE scores\n",
    "    results = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    # Extract ROUGE-2 scores\n",
    "    rouge2 = results[\"rouge2\"]\n",
    "    return {\"rouge2\": round(rouge2.fmeasure * 100, 2)}  # Return F1 score as percentage\n",
    "\n",
    "# Load T5 model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46d5ed50-8140-47d7-a0ac-8cde44aac618",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/79170 [00:00<?, ? examples/s]D:\\Develope\\installation\\anaconda3\\envs\\machine_learning\\lib\\site-packages\\transformers\\tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "                                                                  \r"
     ]
    }
   ],
   "source": [
    "train_dataset = Dataset.from_pandas(train_df_copy)\n",
    "validation_dataset = Dataset.from_pandas(validation_df_copy)\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "validation_dataset = validation_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bebaeffb-747c-45a4-b2b6-8298189a022c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Develope\\installation\\anaconda3\\envs\\machine_learning\\lib\\site-packages\\transformers\\training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7425' max='7425' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7425/7425 33:38, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>4.213700</td>\n",
       "      <td>0.865599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.870700</td>\n",
       "      <td>0.781015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.779300</td>\n",
       "      <td>0.764451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.757956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.741100</td>\n",
       "      <td>0.753764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.739500</td>\n",
       "      <td>0.751063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.737400</td>\n",
       "      <td>0.748552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.728400</td>\n",
       "      <td>0.747202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.727700</td>\n",
       "      <td>0.745845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.732100</td>\n",
       "      <td>0.744791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.725400</td>\n",
       "      <td>0.743753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.725700</td>\n",
       "      <td>0.743325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.723400</td>\n",
       "      <td>0.743001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.729500</td>\n",
       "      <td>0.742749</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7425, training_loss=0.9796646477798822, metrics={'train_runtime': 2020.2592, 'train_samples_per_second': 117.564, 'train_steps_per_second': 3.675, 'total_flos': 3.214503126761472e+16, 'train_loss': 0.9796646477798822, 'epoch': 3.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    learning_rate=5e-6,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,\n",
    "    # metric_for_best_model=\"rouge2\",\n",
    "    # greater_is_better=True,\n",
    "    remove_unused_columns=True,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validation_dataset\n",
    "    # compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "279060df-6ad6-4854-b37a-7e7f17e5da9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./fine_tuned__80k_t5_model_5e-6\\\\tokenizer_config.json',\n",
       " './fine_tuned__80k_t5_model_5e-6\\\\special_tokens_map.json',\n",
       " './fine_tuned__80k_t5_model_5e-6\\\\spiece.model',\n",
       " './fine_tuned__80k_t5_model_5e-6\\\\added_tokens.json')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained('./fine_tuned_80k_t5_model_5e-6')\n",
    "tokenizer.save_pretrained('./fine_tuned__80k_t5_model_5e-6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4c7bc04f-840e-4a35-b8d3-31b0e3f4de3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input_for_generation(article):\n",
    "    return f\"{article} </s>\"\n",
    "\n",
    "# Assuming `model` is your fine-tuned model and `tokenizer` is your tokenizer\n",
    "def generate_summary(article, device):\n",
    "    input_text = prepare_input_for_generation(article)\n",
    "    input_ids = tokenizer.encode(input_text, max_length=512, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "    summary_ids = model.generate(\n",
    "        input_ids,\n",
    "        max_length=100,  \n",
    "        top_k=30,              \n",
    "        top_p=1,\n",
    "        do_sample=True,\n",
    "        early_stopping=True,\n",
    "        pad_token_id=tokenizer.pad_token_id )\n",
    "\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    generated_summary = summary.split('</s>')[-1].strip()\n",
    "    return generated_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fea15a42-7b4e-45b7-b998-5acfc0aaaba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Susie Wiles, co-campaign manager for Trump, initially made the request for enhanced measures during a call about two weeks ago with Biden’s chief of staff, Jeff Zients, two sources familiar with the conversation told CNN. Wiles then made the formal request for additional security with acting Secret Service Director Ronald Rowe on September 30, according to an email reviewed by CNN. A third source familiar with Wiles’ call with Zients said Biden’s chief of staff immediately connected Wiles to leadership at the Department of Homeland Security and Secret Service “so she had a direct line.” The source said Zients made clear that the president had directed the Secret Service to provide the highest level of protection for Trump.\n",
      "==============\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Susie Wiles made the request for enhanced measures during a talk over two weeks ago. Wiles, acting Secret service director Ronald Rowe, said she connected Wiles to a position at the Department of homeland security and Secret Service.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_article = \"Susie Wiles, co-campaign manager for Trump, initially made the request for enhanced measures during a call about two weeks ago with Biden’s chief of staff, Jeff Zients, two sources familiar with the conversation told CNN. Wiles then made the formal request for additional security with acting Secret Service Director Ronald Rowe on September 30, according to an email reviewed by CNN. A third source familiar with Wiles’ call with Zients said Biden’s chief of staff immediately connected Wiles to leadership at the Department of Homeland Security and Secret Service “so she had a direct line.” The source said Zients made clear that the president had directed the Secret Service to provide the highest level of protection for Trump.\"\n",
    "print(input_article)\n",
    "print(\"==============\")\n",
    "generate_summary(input_article, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758a9c2b-bf3e-471a-bbc0-d46c683b5e9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
