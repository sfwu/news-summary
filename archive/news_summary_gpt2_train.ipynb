{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd29235-cd06-45aa-8cd2-fff4f1aed189",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2TokenizerFast, Trainer, TrainingArguments, default_data_collator\n",
    "from datasets import Dataset, load_metric\n",
    "import torch\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "import contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac56ac39-2637-4f4d-bdef-5b7181ec7962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataSet Source\n",
    "# https://www.kaggle.com/datasets/gowrishankarp/newspaper-text-summarization-cnn-dailymail/data\n",
    "\n",
    "def load_input_dataset_files():\n",
    "    files = {}\n",
    "    for dirname, _, filenames in os.walk('./dataset'):\n",
    "        for filename in filenames:\n",
    "            fullpath = os.path.join(dirname, filename)\n",
    "            if filename.split(\".\")[-1] == \"csv\":\n",
    "                files[''.join(filename.split(\".\")[:-1])] = pd.read_csv(fullpath)\n",
    "                print(f\"Loaded file: {filename}\")\n",
    "    return files\n",
    "\n",
    "input_files = load_input_dataset_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55accb7b-48f5-46dc-9908-47128b0ff13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_all_df = input_files['train']\n",
    "raw_validation_all_df = input_files['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fda817-7f4c-4ce0-a4f5-de23e2d812d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw_train_all_df.shape)\n",
    "print(raw_validation_all_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f146a1-80e4-4f01-9de5-ce1613683ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76216d01-7f43-40e1-88ba-5c0d2b054250",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_all_df['article_length'] = raw_train_all_df['article'].apply(lambda x: len(x.split()))\n",
    "raw_train_all_df['summary_length'] = raw_train_all_df['highlights'].apply(lambda x: len(x.split()))\n",
    "raw_validation_all_df['article_length'] = raw_validation_all_df['article'].apply(lambda x: len(x.split()))\n",
    "raw_validation_all_df['summary_length'] = raw_validation_all_df['highlights'].apply(lambda x: len(x.split()))\n",
    "print(raw_train_all_df[['article_length', 'summary_length']].describe())\n",
    "\n",
    "raw_train_all_df['article_length'] = pd.to_numeric(raw_train_all_df['article_length'], errors='coerce')\n",
    "raw_train_all_df['summary_length'] = pd.to_numeric(raw_train_all_df['summary_length'], errors='coerce')\n",
    "\n",
    "raw_validation_all_df['article_length'] = pd.to_numeric(raw_validation_all_df['article_length'], errors='coerce')\n",
    "raw_validation_all_df['summary_length'] = pd.to_numeric(raw_validation_all_df['summary_length'], errors='coerce')\n",
    "\n",
    "train_all_df = raw_train_all_df[(raw_train_all_df['article_length'] <= 900) & (raw_train_all_df['summary_length'] <= 900)]\n",
    "validation_all_df = raw_validation_all_df[(raw_validation_all_df['article_length'] <= 900) & (raw_validation_all_df['summary_length'] <= 900)]\n",
    "print(train_all_df[['article_length', 'summary_length']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9327c6b0-9302-4183-9f7e-e10a18276635",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_all_df.shape)\n",
    "print(validation_all_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06131601-37ec-4c7a-867c-fcb20b12e919",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_all_df.sample(n=40000, random_state=42)\n",
    "validation_df = validation_all_df.sample(n=2000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50db5bc2-0c5f-4d1c-987b-fef8e2aa4897",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Duplicate articles in training set:\", train_df.duplicated(subset=['article']).sum())\n",
    "train_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba64b638-bef6-4e1e-93eb-9153ceafd456",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    # Remove leading/trailing whitespace\n",
    "    text = text.strip()\n",
    "    # Replace multiple spaces with single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "def remove_unwanted_characters(text):\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Remove numbers (optional)\n",
    "    # text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove excess punctuation (e.g., !!!, ???)\n",
    "    text = re.sub(r'([!?.])\\1+', r'\\1', text)\n",
    "    \n",
    "    # Remove other unwanted symbols (customize as needed)\n",
    "    # Example: Remove pipe |, tilde ~, caret ^, etc.\n",
    "    text = re.sub(r'[|~^]', '', text)\n",
    "    return text\n",
    "\n",
    "def expand_contractions_text(text):\n",
    "    return contractions.fix(text)\n",
    "\n",
    "def standardize_possessives(text):\n",
    "    # Example: Convert \"US's\" to \"US is\" if appropriate\n",
    "    # Note: This is context-dependent and should be used carefully\n",
    "    text = re.sub(r\"(\\b\\w+)'s\\b\", r\"\\1 is\", text)\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def format_all(df_list):\n",
    "    for df in df_list:\n",
    "\n",
    "        df.drop_duplicates(subset=['article', 'highlights'], inplace=True)\n",
    "    \n",
    "        # change it's to it is etc\n",
    "        df['article'] = df['article'].apply(expand_contractions_text)\n",
    "        df['highlights'] = df['highlights'].apply(expand_contractions_text)\n",
    "\n",
    "        df['article'] = df['article'].apply(standardize_possessives)\n",
    "        df['highlights'] = df['highlights'].apply(standardize_possessives)\n",
    "        \n",
    "        df['article'] = df['article'].apply(normalize_text)\n",
    "        df['highlights'] = df['highlights'].apply(normalize_text)\n",
    "        \n",
    "        df['article'] = df['article'].apply(remove_unwanted_characters)\n",
    "        df['highlights'] = df['highlights'].apply(remove_unwanted_characters)\n",
    "\n",
    "        df['article'] = df['article'].str.lower()\n",
    "        df['highlights'] = df['highlights'].str.lower()\n",
    "\n",
    "        # df['article'] = df['article'].apply(remove_stopwords)\n",
    "        # df['highlights'] = df['highlights'].apply(remove_stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8d0028-a992-4055-8f4a-0a21187f0e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_copy = train_df.copy()\n",
    "validation_df_copy = validation_df.copy()\n",
    "# train_df_copy = pd.DataFrame(train_df.head(10000))\n",
    "# validation_df_copy = pd.DataFrame(validation_df.head(500))\n",
    "\n",
    "format_all([train_df_copy, validation_df_copy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4276b53b-4642-4594-a3d5-cfe9a0e9aa27",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 9999\n",
    "train_df_copy.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0a8568-6d4e-42da-8b15-98a0125dce93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_special_tokens():\n",
    "\t\"\"\" Returns GPT2 tokenizer after adding separator and padding tokens \"\"\"\n",
    "\ttokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\tspecial_tokens = {'pad_token':'<|pad|>','sep_token':'<|sep|>'}\n",
    "\tnum_add_toks = tokenizer.add_special_tokens(special_tokens)\n",
    "\treturn tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb84a20-c389-4210-a109-fda5094e4abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the GPT-2 tokenizer and model\n",
    "# Move the model to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer = add_special_tokens()\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "ignore_idx = tokenizer.pad_token_id\n",
    "model.to(device)\n",
    "\n",
    "def prepare_input_text(article, summary):\n",
    "    return f\"{article} </s> {summary}\"\n",
    "\n",
    "def generate_input_text_for_all(df_list):\n",
    "    for df in df_list:\n",
    "        df['input_text'] = df.apply(lambda x: prepare_input_text(x['article'], x['highlights']), axis=1)\n",
    "\n",
    "def tokenize_function(df):\n",
    "    encodings = tokenizer(df['input_text'],\n",
    "                        padding='max_length',\n",
    "                        truncation=True,\n",
    "                        max_length=800\n",
    "                    )\n",
    "    # Create labels, setting padding tokens to -100 for loss calculation\n",
    "    encodings['labels'] = [[-100 if token == ignore_idx else token for token in input_ids] for input_ids in encodings['input_ids']]\n",
    "    return encodings\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    rouge = load_metric(\"rouge\")\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    batch_size = 10  # Adjust based on memory capacity\n",
    "    rouge_results = []\n",
    "\n",
    "    # Process ROUGE calculation in smaller batches\n",
    "    for i in range(0, len(decoded_preds), batch_size):\n",
    "        batch_preds = decoded_preds[i:i + batch_size]\n",
    "        batch_labels = decoded_labels[i:i + batch_size]\n",
    "        result = rouge.compute(predictions=batch_preds, references=batch_labels, use_stemmer=True)\n",
    "        rouge_results.append(result)\n",
    "\n",
    "    # Aggregate results\n",
    "    final_result = {\"rouge2\": {\"fmeasure\": np.mean([r[\"rouge2\"].mid.fmeasure for r in rouge_results])}}\n",
    "    return final_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d5ed50-8140-47d7-a0ac-8cde44aac618",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_input_text_for_all([train_df_copy, validation_df_copy])\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df_copy[['input_text']])\n",
    "validation_dataset = Dataset.from_pandas(validation_df_copy[['input_text']])\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "validation_dataset = validation_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "validation_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebaeffb-747c-45a4-b2b6-8298189a022c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=1000,\n",
    "    learning_rate=7e-5,\n",
    "    per_device_train_batch_size=6,\n",
    "    per_device_eval_batch_size=6,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,\n",
    "    # load_best_model_at_end=True,\n",
    "    # metric_for_best_model=\"rouge2\",\n",
    "    # greater_is_better=True,\n",
    "    remove_unused_columns=False\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validation_dataset,\n",
    "    # compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279060df-6ad6-4854-b37a-7e7f17e5da9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('./fine_tuned_40k_model_LR2')\n",
    "tokenizer.save_pretrained('./fine_tuned_40k_model_LR2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7bc04f-840e-4a35-b8d3-31b0e3f4de3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input_for_generation(article):\n",
    "    return f\"{article} </s>\"\n",
    "\n",
    "# Assuming `model` is your fine-tuned model and `tokenizer` is your tokenizer\n",
    "def generate_summary(article, device):\n",
    "    # Prepare the input\n",
    "    input_text = prepare_input_for_generation(article)\n",
    "    \n",
    "    # Tokenize the input\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
    "\n",
    "    # Generate summary\n",
    "    summary_ids = model.generate(\n",
    "        input_ids,\n",
    "        max_length=280,  # Set maximum length for the generated summary\n",
    "        top_k=10,              # Limit the number of highest probability tokens\n",
    "        top_p=0.7,             # Use nucleus sampling\n",
    "        early_stopping=True,\n",
    "        pad_token_id=tokenizer.eos_token_id  # Ensure padding is handled correctly\n",
    "    )\n",
    "\n",
    "    # Decode the generated summary\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Remove the input text to show only the generated summary\n",
    "    generated_summary = summary.split('</s>')[-1].strip()  # Get the part after the separator\n",
    "    return generated_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea15a42-7b4e-45b7-b998-5acfc0aaaba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_article = \"the bishop of the fargo catholic diocese in north dakota has exposed potentially hundreds of church members in fargo, grand forks and jamestown to the hepatitis a virus in late september and early october. the state health department has issued an advisory of exposure for anyone who attended five churches and took communion. bishop john folda (pictured) of the fargo catholic diocese in north dakota has exposed potentially hundreds of church members in fargo, grand forks and jamestown to the hepatitis a . state immunization program manager molly howell says the risk is low, but officials feel it is important to alert people to the possible exposure. the diocese announced on monday that bishop john folda is taking time off after being diagnosed with hepatitis a. the diocese says he contracted the infection through contaminated food while attending a conference for newly ordained bishops in italy last month. symptoms of hepatitis a include fever, tiredness, loss of appetite, nausea and abdominal discomfort. fargo catholic diocese in north dakota (pictured) is where the bishop is located .\"\n",
    "generate_summary(input_article, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ddd91e-6cfe-4da5-b370-318a9a26e5bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2587257c-90c2-452f-a58a-0fd7535a717a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
